# important
#type=int, default=[0], help="dataset train towns (default: [0])" from 1 to 10
# used in Class CarlaMVDetDataset in carla_dataset.py to load the training data in which towns.
train-towns:
#type=int, default=[0], help="dataset train weathers (default: [0])" from 1 to 20
# used in Class CarlaMVDetDataset in carla_dataset.py to load the training data in which weathers.
train-weathers:
#type=int, default=[1], help="dataset validation towns (default: [1])" from 1 to 10
# used in Class CarlaMVDetDataset in carla_dataset.py to load the test data in which towns.
val-towns:
#type=int, default=[1], help="dataset validation weathers (default: [1])" from 1 to 20
# used in Class CarlaMVDetDataset in carla_dataset.py to load the test data in which weathers.
val-weathers:
# actually, there is no flag to distinguish training or test data in CarlaMVDetDataset.
#default=False, help="load lidar data in the dataset",
# used in Class CarlaMVDetDataset in carla_dataset.py to load the lidar data.
with-lidar: True
#type=float, default=5e-4, help="The learning rate for backbone"
# used in func get_optimizer in drivetrainer.py v(0.0003)
# should combine with parameters.optimizer.lr in plato config
backbone-lr:
#default=False, help="The learning rate for backbone is set as backbone-lr"
# should combine with parameters.optimizer.lr in plato config v
with-backbone-lr:
#help="path to dataset"
# used in Class CarlaMVDetDataset in carla_dataset.py
data_dir:
#metavar="NAME", default="newcarla", help="dataset type (default: ImageFolder/ImageTar if empty)"
# used in lass CarlaMVDetDataset in carla_dataset.py v(carla)
dataset:
#default="resnet101", type=str, metavar="MODEL", help='Name of model to train (default: "countception"'
# used in create_model in auto_driving.py v(memfuser_baseline_e1d3)
model:
#type=int, default=32, metavar="N", help="input batch size for training (default: 32)",
# used in func create_carla_loader in drivetrainer.py to set the batch_size of loader
# should combine with trainer.batch_size in plato
batch-size:
# Optimizer parameters
#default="sgd", type=str, metavar="OPTIMIZER", help='Optimizer (default: "sgd"'
# used in function create_optimizer_v2 in drivetrainer.py to set optimizer v(adamw)
# should combine with trainer.optimizer
opt:
#default=None, type=float, metavar="EPSILON", help="Optimizer Epsilon (default: None, use opt default)"
# used in function create_optimizer_v2 in drivetrainer.py to set optimizer v(1e-8)
opt-eps:
#default=None, type=float, metavar="BETA", help="Optimizer Betas (default: None, use opt default)"
#### optional
opt-betas:
#type=float, default=0.9, metavar="M", help="Optimizer momentum (default: 0.9)"
momentum:
#type=float, default=0.0001, help="weight decay (default: 0.0001)" v(0.05)
weight-decay:




# not used
#default=False help="StarAt with pretrained version of specified network (if avail)"
saver-decreasing:
#type=int, default=1, help="Number of frames of the input"
# used in Class CarlaMVDetDataset in carla_dataset.py to  (datasource.py). x not used
temporal-frames:
#type=int, default=-1, help="Number of freeze layers in the backbone"
# used in func create_model in model.factory.py, but it's not used. v(-1)
freeze-num:
#default="train", help="dataset train split (default: train)" not used
train-split:
#default="validation", help="dataset validation split (default: validation)", not used
val-split:
#default="", type=str, metavar="PATH", help="Initialize model from this checkpoint (default: none)"  not used
initial-checkpoint:
#action="store_true",default=False,help="prevent resume of optimizer state when resuming model",
no-resume-opt:
#type=int, default=None, metavar="N", help="number of label classes (Model default if None)"
num-classes:
#default=None, type=str, metavar="POOL", help="Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None."
# used in create_model in auto_driving.py not used
gp:
#type=int, default=None, metavar="N", help="Image patch size (default: None => model default)"
img-size:
#default=None, type=float, metavar="N", help="Input image center crop percent (for validation only)",
crop-pct:
#type=float, nargs="+", default=None, metavar="MEAN", help="Override mean pixel value of dataset",
# used in drivetrainer.py, but the mean and std are not loaded from config but from timm/data/constants.py
mean:
#type=float, default=None, metavar="STD", help="Override std deviation of of dataset",
std:
#default="", type=str, metavar="NAME", help="Image resize interpolation type (overrides model)"
# used in func create_carla_loader in drivetrainer.py to config rgb_transformer,
# but in func rgb_transformer, 'interpolation' is set up as 'bilinear' and not used.
interpolation:
#type=int, default=1, metavar="N", help="ratio of validation batch size to training batch size (default: 1)"
# used in func create_carla_loader (test loader) in drivetrainer.py, but not used
validation-batch-size-multiplier:
#type=float, default=0.0,
# used in func create_carla_dataset in datasource.py, but not used.
augment-prob:






# not sure
#default=False, help="load segmentation data in the dataset"
# used in Class CarlaMVDetDataset in carla_dataset.py to load the segmentation data stored in data["seg"]. x
with-seg:

#default=False, help="load depth data in the dataset"
# used in Class CarlaMVDetDataset in carla_dataset.py to load the depth data stored in data["depth"]. x
with-depth:

#default=False, help="load multi-view data in the dataset"
# used in Class CarlaMVDetDataset in carla_dataset.py to load the multi-view data (datasource.py). v
# it's usually used with 'with-seg' and 'with-depth', but it can also be used independently
#  with 'multi_view_transform' to transform the raw rgb data.
multi-view:

#default=None, nargs=3, type=int, metavar="N N N", help="Input all image dimensions (d h w, e.g. --input-size 3 224 224) for left- right- or rear- view"
# used in func create_carla_loader in carla_loader.py to set dataset.multi_view_transform in func create_carla_rgb_transform in transforms_carla_factory.py
# 3 128 128  v
multi-view-input-size:

# Dataset / Model parameters
#default=False, help="Start with pretrained version of specified network (if avail)",
# used in create_model in auto_driving.py not found in used. The pretrained attribution for Resnet50/26 is True. v
pretrained:

# default="", type=str, metavar="PATH", help="Resume full model and optimizer state from checkpoint (default: none)",
# only used in train_pretrain.py, but not combine with plato
resume:

#default=None, nargs=3, type=int, metavar="N N N", help="Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty",
# used in func create_carla_loader in drivetrainer.py to create rgb_transformer
# similar with multi-view-input-size x
input-size:

#type=float, default=None, metavar="NORM", help="Clip gradient norm (default: None, no clipping)",
# used in func train_one_epoch in drivetrainer.py. if loss_scaler is not none, clip-grad is used. v(5)
# in train_pretrain.py, when 'use_amp' is set, loss_scaler is not none.
clip-grad:

#type=str, default="norm", help='Gradient clipping mode. One of ("norm", "value", "agc")'
# similar with clip-grad. x
clip-mode:

# Learning rate schedule parameters
# default="cosine", type=str, metavar="SCHEDULER", help='LR scheduler (default: "step"'
#第一次调用是在scheduler_factory.py中的create_scheduler_v2函数中，默认为cosine
sched:
#type=float, default=5e-4, metavar="LR", help="learning rate (default: 0.01)"
#第一次调用是在drivetrainer.py的get_optimizer中使用，在计算了linear_scaled_lr，对args.lr重新进行了赋值
lr:
#type=float, nargs="+", default=None, metavar="pct, pct", help="learning rate noise on/off epoch percentages"
lr-noise:
#type=float, default=0.67, metavar="PERCENT", help="learning rate noise limit percent (default: 0.67)"
lr-noise-pct:
#type=float, default=1.0, metavar="STDDEV", help="learning rate noise std-dev (default: 1.0)"
lr-noise-std:
#type=float, default=1.0, help="learning rate cycle len multiplier (default: 1.0)"
lr-cycle-mul:
#type=int, default=1, help="learning rate cycle limit"
lr-cycle-limit:
#type=float, default=5e-6, help="warmup learning rate (default: 0.0001)",
warmup-lr:
#type=float, default=1e-5, metavar="LR", help="lower lr bound for cyclic schedulers that hit 0 (1e-5)"
min-lr:
#type=int, default=200, help="number of epochs to train (default: 2)"
epochs:
#type=int, default=5, help="epochs to warmup LR, if scheduler supports"
warmup-epochs:
#type=float, default=30, help="epoch interval to decay LR"
decay-epochs:
#type=int, default=10, help="epochs to cooldown LR at min_lr, after cyclic schedule ends"
cooldown-epochs:
#type=int, default=10, help="patience epochs for Plateau LR scheduler (default: 10"
patience-epochs:
#type=float, default=0.1, help="LR decay rate (default: 0.1)"
decay-rate:
###这上面所有和lrcheduler有关的参数都是在scheduler_factory.py中的create_scheduler_v2函数中使用的,并存在scheduler_kwargs这个函数里面，有默认值，而且没有在其他地方重新赋值

#type=float, default=0.0, help="epoch repeat multiplier (number of times to repeat dataset epoch per train epoch)."
#没有找到这个参数在哪里使用
epoch-repeats:
#default=None, type=int, help="manual epoch number (useful on restarts)"
#没有找到这个参数在哪里使用
start-epoch:





# Augmentation & regularization parameters
#default=False, help="Disable all training augmentation, override other train aug args"
no-aug:
#type=float, default=[0.08, 1.0], metavar="PCT", help="Random resize scale (default: 0.08 1.0)",
scale:
#type=float, default=[3.0 / 4.0, 4.0 / 3.0], metavar="RATIO", help="Random resize aspect ratio (default: 0.75 1.33)"
ratio:
#type=float, default=0.5, help="Horizontal flip training aug probability"
hflip:
#type=float, default=0.0, help="Vertical flip training aug probability"
vflip:
#type=float, default=0.1, metavar="PCT", help="Color jitter factor (default: 0.4)"
color-jitter:
#type=str, default=None, help='Use AutoAugment policy. "v0" or "original". (default: None)'
aa:
#type=int, default=0, help="Number of augmentation splits (default: 0, valid: 0 or >=2)"
aug-splits:
#default=False, help="Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`."
jsd:
#type=float, default=0.0, metavar="PCT", help="Random erase prob (default: 0.)"
reprob:
#type=str, default="const", help='Random erase mode (default: "const")'
remode:
#type=int, default=1, help="Random erase count (default: 1)"
recount:
#default=False, help="Do not random erase first (clean) augmentation split"
resplit:
#type=float, default=0.0, help="Label smoothing (default: 0.0)"
smoothing:
#default=False, action='store_true', help="L1 smooth"
smoothed_l1:
#default="random", help='Training interpolation (random, bilinear, bicubic default: "random")'
train-interpolation:
#type=float, default=0.0, metavar="PCT", help="Dropout rate (default: 0.)"
drop:
#type=float, default=None, help="Drop connect rate, DEPRECATED, use drop-path (default: None)"
drop-connect:
#type=float, default=0.1, help="Drop path rate (default: None)"
drop-path:
#type=float, default=None, help="Drop block rate (default: None)"
drop-block:

# Batch norm parameters (only works with gen_efficientnet based models currently)

#default=False, help="Use Tensorflow BatchNorm defaults for models that support it (default: False)"
bn-tf:
#default=None, help="BatchNorm momentum override (if not None)"
bn-momentum:
#default=None, help="BatchNorm epsilon override (if not None)"
bn-eps:
#help="Enable NVIDIA Apex or Torch synchronized BatchNorm."
sync-bn:
#default="", help='Distribute BatchNorm stats between nodes after each epoch ("broadcast", "reduce", or "")'
dist-bn:
#help="Enable separate BN layers per augmentation split."
split-bn:

# Model Exponential Moving Average

#default=False, help="Enable tracking moving average of model weights"
model-ema:
#default=False, help="Force ema to be tracked on CPU, rank=0 node only. Disables EMA validation.",
model-ema-force-cpu:
#default=0.9998, help="decay factor for model weights moving average (default: 0.9998)"
model-ema-decay:

# Misc
#type=int, default=42, metavar="S", help="random seed (default: 42)"
seed:
#default=50, help="how many batches to wait before logging training status"
log-interval:
#default=0, help="how many batches to wait before writing recovery checkpoint",
recovery-interval:
#default=5, help="number of checkpoints to keep (default: 10)"
checkpoint-hist:
#default=4, help="how many training processes to use (default: 1)"
workers:
#default=False, help="save images of input bathes every log interval for debugging"
save-images:
# default=False, help="use NVIDIA Apex AMP or Native AMP for mixed precision training"
amp:
#default=False, help="Use NVIDIA Apex AMP mixed precision"
apex-amp:
#default=False, help="Use Native Torch AMP mixed precision"
native-amp:
#default=False, help="Use channels_last memory layout"
channels-last:
#default=False, help="Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU."
pin-mem:
#default=False, help="disable fast prefetcher"
no-prefetcher:
#help="path to output folder (default: none, current dir)"
output:
#default="", help="name of train experiment, name of sub-folder for output"
experiment:
#default="top1", type=str,  metavar="EVAL_METRIC", help='Best metric (default: "top1"'
eval-metric:
#default=0, help="Test/inference time augmentation (oversampling) factor. 0=None (default: 0)"
tta:
#default=0, type=int
local-rank:
#default=False, help="use the multi-epochs-loader to save time at the beginning of every epoch"
use-multi-epochs-loader:
#help="convert model torchscript for inference"
torchscript:
#default=False, help="log training and validation metrics to wandb"
log-wandb:



